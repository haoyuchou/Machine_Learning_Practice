{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "# use the other three algorithms to do hard vote\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "# random state for same data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.888\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('svc', SVC(probability=True, random_state=42))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# soft voting\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.92\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "# help the unicode for BaggingClassifier prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.912, 0.928, 0.928])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bag_clf, X_train, y_train, cv=3)\n",
    "# not so bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try pasting\n",
    "past_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=False, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.904, 0.928, 0.92 ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(past_clf, X_train, y_train, cv=3)\n",
    "# it seems bagging is slightly better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, oob_score=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9253333333333333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_\n",
    "# it is likely to achieve 92% on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35579515, 0.64420485],\n",
       "       [0.43513514, 0.56486486],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01030928, 0.98969072],\n",
       "       [0.03174603, 0.96825397],\n",
       "       [0.07672634, 0.92327366],\n",
       "       [0.39189189, 0.60810811],\n",
       "       [0.06145251, 0.93854749],\n",
       "       [0.92689295, 0.07310705],\n",
       "       [0.88205128, 0.11794872],\n",
       "       [0.59850374, 0.40149626],\n",
       "       [0.04896907, 0.95103093],\n",
       "       [0.7565445 , 0.2434555 ],\n",
       "       [0.81377551, 0.18622449],\n",
       "       [0.88528678, 0.11471322],\n",
       "       [0.07407407, 0.92592593],\n",
       "       [0.04738155, 0.95261845],\n",
       "       [0.92051282, 0.07948718],\n",
       "       [0.69974555, 0.30025445],\n",
       "       [0.94358974, 0.05641026],\n",
       "       [0.06100796, 0.93899204],\n",
       "       [0.224     , 0.776     ],\n",
       "       [0.9125964 , 0.0874036 ],\n",
       "       [0.98746867, 0.01253133],\n",
       "       [0.95967742, 0.04032258],\n",
       "       [0.        , 1.        ],\n",
       "       [0.94255875, 0.05744125],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03466667, 0.96533333],\n",
       "       [0.7020202 , 0.2979798 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01262626, 0.98737374],\n",
       "       [0.07772021, 0.92227979],\n",
       "       [0.09350649, 0.90649351],\n",
       "       [0.97889182, 0.02110818],\n",
       "       [0.01827676, 0.98172324],\n",
       "       [0.53191489, 0.46808511],\n",
       "       [0.02122016, 0.97877984],\n",
       "       [0.98979592, 0.01020408],\n",
       "       [0.10242588, 0.89757412],\n",
       "       [0.33773087, 0.66226913],\n",
       "       [0.98684211, 0.01315789],\n",
       "       [0.98714653, 0.01285347],\n",
       "       [0.00755668, 0.99244332],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.05691057, 0.94308943],\n",
       "       [0.97727273, 0.02272727],\n",
       "       [0.05420054, 0.94579946],\n",
       "       [0.9443038 , 0.0556962 ],\n",
       "       [0.78740157, 0.21259843],\n",
       "       [0.92467532, 0.07532468],\n",
       "       [0.81794195, 0.18205805],\n",
       "       [0.01758794, 0.98241206],\n",
       "       [0.09511568, 0.90488432],\n",
       "       [0.78296703, 0.21703297],\n",
       "       [0.01897019, 0.98102981],\n",
       "       [0.01344086, 0.98655914],\n",
       "       [0.01492537, 0.98507463],\n",
       "       [0.82170543, 0.17829457],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.71900826, 0.28099174],\n",
       "       [0.9921875 , 0.0078125 ],\n",
       "       [0.01049869, 0.98950131],\n",
       "       [0.7513369 , 0.2486631 ],\n",
       "       [0.97727273, 0.02272727],\n",
       "       [0.99230769, 0.00769231],\n",
       "       [0.60367454, 0.39632546],\n",
       "       [0.98461538, 0.01538462],\n",
       "       [0.35824742, 0.64175258],\n",
       "       [0.30666667, 0.69333333],\n",
       "       [0.41621622, 0.58378378],\n",
       "       [0.72922252, 0.27077748],\n",
       "       [0.        , 1.        ],\n",
       "       [0.25      , 0.75      ],\n",
       "       [0.9015544 , 0.0984456 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0302267 , 0.9697733 ],\n",
       "       [0.95844156, 0.04155844],\n",
       "       [0.00512821, 0.99487179],\n",
       "       [0.18441558, 0.81558442],\n",
       "       [0.13554987, 0.86445013],\n",
       "       [0.40502793, 0.59497207],\n",
       "       [0.98704663, 0.01295337],\n",
       "       [0.04381443, 0.95618557],\n",
       "       [0.67307692, 0.32692308],\n",
       "       [0.07341772, 0.92658228],\n",
       "       [0.01578947, 0.98421053],\n",
       "       [0.        , 1.        ],\n",
       "       [0.38046272, 0.61953728],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01754386, 0.98245614],\n",
       "       [0.05277045, 0.94722955],\n",
       "       [0.01028278, 0.98971722],\n",
       "       [0.80851064, 0.19148936],\n",
       "       [0.7115903 , 0.2884097 ],\n",
       "       [0.07407407, 0.92592593],\n",
       "       [1.        , 0.        ],\n",
       "       [0.34473684, 0.65526316],\n",
       "       [0.66402116, 0.33597884],\n",
       "       [0.01542416, 0.98457584],\n",
       "       [0.12266667, 0.87733333],\n",
       "       [0.42746114, 0.57253886],\n",
       "       [0.97142857, 0.02857143],\n",
       "       [0.03899721, 0.96100279],\n",
       "       [0.97493734, 0.02506266],\n",
       "       [0.44235925, 0.55764075],\n",
       "       [0.27968338, 0.72031662],\n",
       "       [0.9974026 , 0.0025974 ],\n",
       "       [0.24403183, 0.75596817],\n",
       "       [0.85233161, 0.14766839],\n",
       "       [0.26329114, 0.73670886],\n",
       "       [0.77653631, 0.22346369],\n",
       "       [0.9893617 , 0.0106383 ],\n",
       "       [0.98663102, 0.01336898],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.48849105, 0.51150895],\n",
       "       [0.99162011, 0.00837989],\n",
       "       [0.06793478, 0.93206522],\n",
       "       [0.9895288 , 0.0104712 ],\n",
       "       [0.97704082, 0.02295918],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95561358, 0.04438642],\n",
       "       [0.97777778, 0.02222222],\n",
       "       [0.03580563, 0.96419437],\n",
       "       [0.95760599, 0.04239401],\n",
       "       [0.96508728, 0.03491272],\n",
       "       [0.02887139, 0.97112861],\n",
       "       [0.23306233, 0.76693767],\n",
       "       [0.85529716, 0.14470284],\n",
       "       [0.4015544 , 0.5984456 ],\n",
       "       [0.91948052, 0.08051948],\n",
       "       [0.002457  , 0.997543  ],\n",
       "       [0.0265252 , 0.9734748 ],\n",
       "       [0.82849604, 0.17150396],\n",
       "       [0.76863753, 0.23136247],\n",
       "       [0.5390625 , 0.4609375 ],\n",
       "       [0.88664987, 0.11335013],\n",
       "       [0.93814433, 0.06185567],\n",
       "       [0.1171875 , 0.8828125 ],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [0.08136483, 0.91863517],\n",
       "       [0.01282051, 0.98717949],\n",
       "       [0.1227154 , 0.8772846 ],\n",
       "       [0.73969072, 0.26030928],\n",
       "       [0.96946565, 0.03053435],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03403141, 0.96596859],\n",
       "       [0.00265957, 0.99734043],\n",
       "       [0.0620155 , 0.9379845 ],\n",
       "       [0.02325581, 0.97674419],\n",
       "       [0.9924812 , 0.0075188 ],\n",
       "       [0.98373984, 0.01626016],\n",
       "       [0.86449864, 0.13550136],\n",
       "       [0.99730458, 0.00269542],\n",
       "       [1.        , 0.        ],\n",
       "       [0.87598945, 0.12401055],\n",
       "       [0.00775194, 0.99224806],\n",
       "       [0.64925373, 0.35074627],\n",
       "       [0.32994924, 0.67005076],\n",
       "       [0.07336957, 0.92663043],\n",
       "       [0.01534527, 0.98465473],\n",
       "       [0.38961039, 0.61038961],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97554348, 0.02445652],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.07027027, 0.92972973],\n",
       "       [0.00520833, 0.99479167],\n",
       "       [0.92553191, 0.07446809],\n",
       "       [0.02077922, 0.97922078],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.04347826, 0.95652174],\n",
       "       [0.82994924, 0.17005076],\n",
       "       [0.90526316, 0.09473684],\n",
       "       [0.033241  , 0.966759  ],\n",
       "       [0.94559585, 0.05440415],\n",
       "       [0.90185676, 0.09814324],\n",
       "       [0.9611399 , 0.0388601 ],\n",
       "       [0.01312336, 0.98687664],\n",
       "       [0.01856764, 0.98143236],\n",
       "       [0.99212598, 0.00787402],\n",
       "       [0.24427481, 0.75572519],\n",
       "       [0.98958333, 0.01041667],\n",
       "       [0.12634409, 0.87365591],\n",
       "       [0.01808786, 0.98191214],\n",
       "       [0.98969072, 0.01030928],\n",
       "       [0.        , 1.        ],\n",
       "       [0.19945355, 0.80054645],\n",
       "       [0.88713911, 0.11286089],\n",
       "       [0.90600522, 0.09399478],\n",
       "       [0.61741425, 0.38258575],\n",
       "       [0.67733333, 0.32266667],\n",
       "       [0.03826531, 0.96173469],\n",
       "       [0.2421875 , 0.7578125 ],\n",
       "       [0.98933333, 0.01066667],\n",
       "       [0.92875989, 0.07124011],\n",
       "       [0.9171123 , 0.0828877 ],\n",
       "       [0.98387097, 0.01612903],\n",
       "       [0.04232804, 0.95767196],\n",
       "       [0.01041667, 0.98958333],\n",
       "       [0.09974425, 0.90025575],\n",
       "       [0.5127551 , 0.4872449 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02046036, 0.97953964],\n",
       "       [0.97474747, 0.02525253],\n",
       "       [0.08918919, 0.91081081],\n",
       "       [0.12144703, 0.87855297],\n",
       "       [0.88549618, 0.11450382],\n",
       "       [0.04557641, 0.95442359],\n",
       "       [0.37073171, 0.62926829],\n",
       "       [0.01355014, 0.98644986],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01302083, 0.98697917],\n",
       "       [0.01369863, 0.98630137],\n",
       "       [0.91052632, 0.08947368],\n",
       "       [0.9012987 , 0.0987013 ],\n",
       "       [0.95897436, 0.04102564],\n",
       "       [0.0188172 , 0.9811828 ],\n",
       "       [0.05670103, 0.94329897],\n",
       "       [0.96524064, 0.03475936],\n",
       "       [0.11671088, 0.88328912],\n",
       "       [0.        , 1.        ],\n",
       "       [0.22955145, 0.77044855],\n",
       "       [0.97333333, 0.02666667],\n",
       "       [0.84594595, 0.15405405],\n",
       "       [0.11948052, 0.88051948],\n",
       "       [0.71621622, 0.28378378],\n",
       "       [0.92838875, 0.07161125],\n",
       "       [0.15860215, 0.84139785],\n",
       "       [0.13953488, 0.86046512],\n",
       "       [0.98982188, 0.01017812],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01358696, 0.98641304],\n",
       "       [0.01315789, 0.98684211],\n",
       "       [0.38324873, 0.61675127],\n",
       "       [0.85263158, 0.14736842],\n",
       "       [0.04113111, 0.95886889],\n",
       "       [0.9893617 , 0.0106383 ],\n",
       "       [0.85236769, 0.14763231],\n",
       "       [0.0025641 , 0.9974359 ],\n",
       "       [0.76363636, 0.23636364],\n",
       "       [0.98737374, 0.01262626],\n",
       "       [0.00527704, 0.99472296],\n",
       "       [0.98971722, 0.01028278],\n",
       "       [0.06182796, 0.93817204],\n",
       "       [0.01044386, 0.98955614],\n",
       "       [0.11653117, 0.88346883],\n",
       "       [0.24274406, 0.75725594],\n",
       "       [0.8956743 , 0.1043257 ],\n",
       "       [0.06169666, 0.93830334],\n",
       "       [0.98694517, 0.01305483],\n",
       "       [0.59850374, 0.40149626],\n",
       "       [0.08080808, 0.91919192],\n",
       "       [0.616     , 0.384     ],\n",
       "       [0.88688946, 0.11311054],\n",
       "       [0.00787402, 0.99212598],\n",
       "       [0.99492386, 0.00507614],\n",
       "       [0.01041667, 0.98958333],\n",
       "       [0.        , 1.        ],\n",
       "       [0.77114428, 0.22885572],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98918919, 0.01081081],\n",
       "       [0.10649351, 0.89350649],\n",
       "       [0.73846154, 0.26153846],\n",
       "       [0.13513514, 0.86486486],\n",
       "       [0.9972973 , 0.0027027 ],\n",
       "       [0.90104167, 0.09895833],\n",
       "       [0.01285347, 0.98714653],\n",
       "       [0.05540897, 0.94459103],\n",
       "       [0.13350785, 0.86649215],\n",
       "       [0.08695652, 0.91304348],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96899225, 0.03100775],\n",
       "       [0.84615385, 0.15384615],\n",
       "       [0.15013405, 0.84986595],\n",
       "       [0.93384224, 0.06615776],\n",
       "       [0.04221636, 0.95778364],\n",
       "       [0.61265823, 0.38734177],\n",
       "       [0.13917526, 0.86082474],\n",
       "       [0.95064935, 0.04935065],\n",
       "       [0.90027701, 0.09972299],\n",
       "       [0.00789474, 0.99210526],\n",
       "       [0.94041451, 0.05958549],\n",
       "       [0.8987013 , 0.1012987 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05053191, 0.94946809],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03183024, 0.96816976],\n",
       "       [0.98963731, 0.01036269],\n",
       "       [0.09189189, 0.90810811],\n",
       "       [0.88235294, 0.11764706],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01066667, 0.98933333],\n",
       "       [0.0458221 , 0.9541779 ],\n",
       "       [0.688     , 0.312     ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.67435897, 0.32564103],\n",
       "       [0.86956522, 0.13043478],\n",
       "       [0.99230769, 0.00769231],\n",
       "       [0.66753927, 0.33246073],\n",
       "       [0.47733333, 0.52266667],\n",
       "       [0.01362398, 0.98637602],\n",
       "       [0.82531646, 0.17468354],\n",
       "       [0.01591512, 0.98408488],\n",
       "       [1.        , 0.        ],\n",
       "       [0.77513228, 0.22486772],\n",
       "       [0.9871134 , 0.0128866 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.84771574, 0.15228426],\n",
       "       [0.27720207, 0.72279793],\n",
       "       [0.1689008 , 0.8310992 ],\n",
       "       [0.2382199 , 0.7617801 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.75065617, 0.24934383],\n",
       "       [0.90649351, 0.09350649],\n",
       "       [0.05882353, 0.94117647],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97837838, 0.02162162],\n",
       "       [0.98992443, 0.01007557],\n",
       "       [0.00507614, 0.99492386],\n",
       "       [0.06887755, 0.93112245],\n",
       "       [0.91282051, 0.08717949],\n",
       "       [0.93782383, 0.06217617],\n",
       "       [1.        , 0.        ],\n",
       "       [0.24129353, 0.75870647],\n",
       "       [0.98933333, 0.01066667],\n",
       "       [0.13      , 0.87      ],\n",
       "       [0.95103093, 0.04896907],\n",
       "       [0.04522613, 0.95477387],\n",
       "       [0.98777506, 0.01222494],\n",
       "       [0.99479167, 0.00520833],\n",
       "       [0.98271605, 0.01728395],\n",
       "       [0.        , 1.        ],\n",
       "       [0.93882979, 0.06117021],\n",
       "       [0.01591512, 0.98408488],\n",
       "       [0.06958763, 0.93041237],\n",
       "       [0.05637255, 0.94362745],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98913043, 0.01086957],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96524064, 0.03475936],\n",
       "       [0.0802139 , 0.9197861 ],\n",
       "       [0.9872449 , 0.0127551 ],\n",
       "       [0.1875    , 0.8125    ],\n",
       "       [0.0156658 , 0.9843342 ],\n",
       "       [0.04569892, 0.95430108],\n",
       "       [0.        , 1.        ],\n",
       "       [0.81693989, 0.18306011],\n",
       "       [0.07518797, 0.92481203],\n",
       "       [0.1292876 , 0.8707124 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.92708333, 0.07291667],\n",
       "       [0.22751323, 0.77248677],\n",
       "       [0.93939394, 0.06060606],\n",
       "       [0.0536193 , 0.9463807 ],\n",
       "       [0.12834225, 0.87165775],\n",
       "       [1.        , 0.        ],\n",
       "       [0.92183288, 0.07816712],\n",
       "       [0.61170213, 0.38829787],\n",
       "       [0.86863271, 0.13136729],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02150538, 0.97849462],\n",
       "       [0.94666667, 0.05333333],\n",
       "       [0.0298103 , 0.9701897 ],\n",
       "       [0.13874346, 0.86125654],\n",
       "       [0.91435768, 0.08564232],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0859375 , 0.9140625 ],\n",
       "       [0.69086022, 0.30913978]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_\n",
    "# it has predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_leaf_nodes=16, n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "# contain hyperparameter of DecisionTree and Bagging\n",
    "# use all CPU\n",
    "# max leaf node\n",
    "rnd_clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)\n",
    "# almost the same as the previous random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=500,n_jobs=-1)\n",
    "# use all CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10879719105604406\n",
      "sepal width (cm) 0.023328309269266743\n",
      "petal length (cm) 0.42478411736048655\n",
      "petal width (cm) 0.4430903823142026\n"
     ]
    }
   ],
   "source": [
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_clf = AdaBoostClassifier(\n",
    "DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "algorithm = \"SAMME.R\", learning_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a second DecisionTreeRegressor on the residual errors\n",
    "# made by the first predictor\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions on\n",
    "#a new instance by adding up the predictions of all the trees\n",
    "X_new = np.array([[0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators = 3, learning_rate= 1)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=120)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [\n",
    "    mean_squared_error(y_val, y_pred)\n",
    "    for y_pred in gbrt.staged_predict(X_val)\n",
    "]\n",
    "bst_n_estimators = np.argmin(errors) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=85)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
    "gbrt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break # early stopping\n",
    "            # early stopping actually happen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xgboost\n",
    "# useful library\n",
    "#xgb_reg = xgboost.XGBRegressor()\n",
    "#xgb_reg.fit(X_train, y_train)\n",
    "#y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_reg.fit(X_train, y_train,\n",
    "#eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "#y_pred = xgb_reg.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
